\chapter{Conclusion}
\label{chapter8}

In this dissertation we examined a particular tree structure that hindered parallel algorithmic performance of one of the state of the art data-parallel contour tree algorithm. These structures are long paths in height trees with a characteristic zig zag pattern. We call them the w-structures. In order to better understand them we developed three algorithms for the detection of the largest w-structure in a height tree. We prooved those algorithm are correct and showed formal bounds on their time and space complexity. We implemented these algorithms and used them to establish the existence of w-structures in contour trees of real life data. By defining a metric on these w-structures we showed empirically that the longest w-structure impacts does impact available parallelism in the data-parallel contour tree algorithm.

In addition to this we explored the use of a tool from Topological Data Analysis called Persistent Homology. Persistent homology is a general framework for topological simplification and were interested in whether it is equivalent to a contour tree specific tool for topological simplification called branch decomposition. We demonstrated that they are not equivalent by computing both on a counter example based on the w-structures.



% We investigated whether Persistent Homology is equivalent to a specialised method for topological simplification of contour trees called Branch Decomposition. Through a counter example based on the w-structures we showed that they are not exactly equivalent.

% * We thus conclude that the w-structures not only pose theoretical and practical problems in a particular algorithm for data-parallel contour tree construction, but they are also a breaking case for other proofs *.

\section{Personal Reflection}

The most difficult part of the project was learning the prerequisite mathematics - Topology, Algebraic Topology and Computational Algebraic Topology. I had covered some of these topics in my second semester with a module on Topology that included an introduction to Algebraic Topology. Throughout the summer the most time consuming part was to learn Homology and to then learn how to apply it via Persistent Homology. The most challenging part was to understand how and why exactly Extended Persistence works. The main difficulty was making sense of the great deal of moving components that enable extended persistence. Those are Morse Theory, Point Set Topology, Homology, Cohomology, Spectral Sequences and the Poincare-Lefschetz duality. In the end I did not have to use all of them for the specific problem I was trying to solve. I did however had to learn how the ideas borrowed from those fields work and relate to one in other in other to pick out exactly the ones I need for my specific proofs.

My approach to learning those fields was not optimal. I overcommited a lot of my time and energy by trying to immediately learn how extended persistence is computed. Without sufficient prerequisite knowledge this attempt was futile and caused nothing but confusion. Henceforth I decided to make my approach more systematic and disciplined. I did this by outlining all of the different ideas, definitions and theorems that build the foundation of extended persistence. I used this to going through all of them in a bottom up fashion. I went through the relevant books and papers slowly and carefully and I made sure I can convince myself of the theory by creating small scale examples and giving myself simple problems to proove. Overal I am satisfied with the results I have obtained. This gave me valuable experience in teaching myself new mathematics and approaching novel research areas.

What I am most proud with in this dissertation is creating and implementing the w-diameter algorithms. I started off with a well defined problem that had not been solved before and I had to come up with an algorihm for it. The first thing I did was to try and come up with definitions that capture the problem statement and allow me to work with it in a formal setting. It took me some experimentation to come up with the characterisation of w-paths via kinks and to realise this characterisation can be used as a metric much like path length. Upon realising this I immediately that it may be possible to modify existing tree diameter algorithms.



The first algorithm I worked on was the 2xBFS algorithm. I started off with just the idea and first implemented it to test whether it holds in practise. Upon obtained obtaining satisfactory results I began to look for ways to modify the proofs of correctness of the original tree diameter algorithm. The most challenging part of constructing the proof was to disect proofs of tree diameter algorithms and see exactly which of their components I have to modify. The most challenging aspects of proving the correctness of 2xBSF was in working around the pathological cases where it did not return the correct output.

% I was puzzled when I realised that there are pathological cases where the algorithm does not return the correct answear. I initially though they could be arbitrarily bad and that would make the algorithm useless. Despite this I managed to formalise the ideas of w-path combinations and give a strict lower bound on the correctness of the algorithm.
%
% The DP alorithm on the other hand proved to be more challenging. I could not figure out exactly how to combine paths that go through the vertex  of the tree and my implementation was not returning the correct resutls. It took me alot of trial and error and manually checking the output of the algorithm on small graphs. In the initial version of the algorithm I was considering all children of children of the root and I could not determine why I was not getting the correct output. Upon formalising the algorithm and attempting the proove its correctness I realised that I had made a wrong assumption. After this I determined that I must only check the children of children that contribute to a maximum w-path.

With the DP algorithm on the other I did not even have a clear idea of how to implement it initially. Through trial and error on numerous small scale example height trees I came up with the formulation of the solution. In the case of this algorithm my mathematical proof preceeded my implementation. It was the case however that my initial proof was not correct and it was though a discrepancy in the output that I noticed and corrected that.

The only part of the dissertation I believe I could have developed more is the empirical study. Initially that chapter was supposed to be the backbone of the dissertation. Especially if I had failed to produce meaningful results with the extended persistence. After commiting so much time to the w-detection algorithms and extended persistence I had already covered too much ground and had little time to produce a more detailed and insighful empirical study.

Overall the project evolved as I was working on it and my ideas for what exacly I would put in it chanded throughout the course of he summer. This approach was more risky than I would have liked, but in the end I was confident in my ability to finish it as I had planned. The project also turned out to be more theoretical than I had originally planned. I am satisfied that this is the case. This gave me the opportunity to advance my mathematical and analytical skills. It has also opened the door for more future research directions and made me more flexible.

% Overall I feel that my approach to doing was not optimal. I struggled in the begining and overcommited alot of my time and energy to it.


\section{Future Work}

Finally we will present possile direction for future work. They will be presented in two part on part related to the empirical study of the w-structures and another to the the relation of extended persistence to the contour tree.



\subsection{Practical Future Directions}

One obvious direction is to extend the empirical study by ontaining more real life data to analyse. This would require gathering and analysing more data.

Can we obtain the w-diameter of a contour tree directly from raw data? Computing the w-diameter of the contour tree itself is useful, but it is like port mortem analysis. If we are able to obtain the w-diameter without constructing the contour tree explicitly we may be able to use the information to speed up the contruction around the w-diameter.

Are there other ways of generating w-structures? Can we devise a general way that would work in any number of dimensions?

We did not have a chance to investigate how different w-structures interact with one another during the merge phase. This will probably be the next best step.

\subsection{Practical Tehoretical Directions}

% There are many unansweared question on the relations of contour tree simplification and persistent homology. For example notice that the way the 0th persistence of an ascending filtration is computed is similar to how a join tree is computer. The similarity is in that joining components correspond to connected components in sublevel sets which in turn correspond to homology classes in the 0th homology. The joining of components coresponds to merging of homology classes. Consequently the persistent pairs correspond to branches in the join tree. If we instead take a descending filtration we obtain a similarity between the split tree and the persistent homology of the descending filtration. Further work can be done in showing whether the computations are equivalent formally and in translating idea from each of the frameworks to the other.

The primary direction this can take is to further explore the connection between contour tree simplification and branch decomposition. A way to do this would be to explore both in a broader context and produce results on whether the two are equivalent in general in the case of join/split trees and descending/descending filtrations.

Another question is whether contour tree simplification can be expressed through another filtration that is not the ascending or descending filtration. Consider for example a simplicial mesh $M$ and its level sets $M_i$. Homology gives us to tools of identifying the connected components of the the $M_i$ by computing the 0th homology. It is possible however to somehow relate the homology classes of different level sets. This would require a sequence of the form

$$ ... \rightarrow H_0(M_i) \rightarrow H_0(M_{i+1}) \rightarrow ... \rightarrow H_0(M_{j}) \rightarrow H_0(M_{j+1}) \rightarrow ...$$

But what would the function between the homology groups be? Can me somehow induce them through function between the sublevel sets themselvs? We certainly cannot use inclusion maps because level sets are not subsets of one another. It is worth contemplating whether this is feasble. Another question is whether it would be practical at all. Even if we do so all we will obtain is a mathematical reformulation of a problem we already have. Will that lead to more efficient algorithms or to better understanding of the subject?

% This is the conclusion.
%
% We have done so many things.
%
% I tried so hard
% And got so far
% But in the end
% It doesn't even matter
% I had to fall
% To lose it all
% But in the end
% It doesn't even matter
