\chapter{Conclusion}
\label{chapter8}

In this dissertation we examined a particular tree structure that hindered parallel algorithmic performance of one of the state of the art data-parallel contour tree algorithm. These structures are long paths in height trees with a characteristic zig zag pattern. We call them the w-structures. In order to better understand them we developed three algorithms for the detection of the largest w-structure in a height tree. We prooved those algorithm are correct and showed formal bounds on their running time. Furthermore we implemented these algorithms and used them to analyse real life data sets to establish the existence of w-structures. In addition to this we explored a tools in Topological Data Analysis called Persistent Homology. This tool was primarily developed for simplifying topological features. We investigated whether Persistent Homology is equivalent to a specialised method for topological simplification of contour trees called Branch Decomposition. Through a counter example based on the w-structures we showed that they are not exactly equivalent.

% * We thus conclude that the w-structures not only pose theoretical and practical problems in a particular algorithm for data-parallel contour tree construction, but they are also a breaking case for other proofs *.

\section{Personal Reflection}

The most difficult part of the project was learning the prerequisite mathematics - Topology, Algebraic Topology and Computational Algebraic Topology. I had covered some of these topics in my second semester with a module on Topology that included an introduction fo Algebraic Topology. Throughout the summer the most time consuming part was to learn Homology and to then learn how to apply it via Persistent Homology. The most challenging part was to understand how and why exactly Extended Persistence works. It was challenging because it is a relatively new research area and there are no good unified references on the subject for students. I had to learn the mathematics behind it from reading the relevant scientific papers and then experimenting with it myself. The main difficulty was making sense of the great deal of moving components that enable extended persistence. Those are Morse Theory, Point Set Topology, Homology, Cohomology, Spectral Sequences and the Poincare-Lefschetz duality. In the end I did not have to use all of them for the specific problem I was trying to solve. I did however had to learn how the ideas borrowed from those fields work and relate to one in other in other to pick out exactly the ones I need for my specific proofs.

My approach to learning those fields was not optimal. I overcommited a lot of my time and energy by trying to immediately learn how Extended Persistence is computed. This resulted in causing nothing but confusion and frustration. In the end I decided to make my approach more systematic and disciplined. I did this by outlining all of the different ideas, definitions and theorems that build the foundation of Extended persistence. Afterwards I started going through all of them in a bottom up fashion. I went through the relevant books and papers slowly and carefully and I made sure I can convince myself of the theory by creating small scale examples and giving myself simple problems to proove. Overal I am satisfied with the results I've obtained. What was more valuable to me was learning how to teach myself new mathematics and how to approach novel research areas.

What I am most proud with in this dissertation is creating and implementing the w-diameter algorithms. I started off with a well defined problem that had not been solved before and I had to come up with an algorihm to solve it myself. The first thing I did was to try and come up with definitions that capture the problem statement and allow me to work with it in a formal setting. It took me quite a bit of experimentation to come up with the characterisation of w-paths via kinks and to realise this characterisation can be used as a metric much like path length. Upon realising this I immediately recognised that it may be possible to modify existing tree diameter algorithms.

The first algorithm I implemented was the 2xBFS algorithm. I had not proven its correctness and I just wanted to see if it would work in practise. When I obtained satisfactory results I began to look for ways to modify the proofs of correctness. The most challenging part of constructing the proof was to disect proofs of tree diameter algorithms and see exactly which of their components I have to modify. I was puzzled when I realised that there are pathological cases where the algorithm does not return the correct answear. I initially though they could be arbitrarily bad and that would make the algorithm useless. Despite this I managed to formalise the ideas of w-path combinations and give a strict lower bound on the correctness of the algorithm.

The DP alorithm on the other hand proved to be more challenging. I could not figure out exactly how to combine paths that go through the vertex  of the tree and my implementation was not returning the correct resutls. It took me alot of trial and error and manually checking the output of the algorithm on small graphs. In the initial version of the algorithm I was considering all children of children of the root and I could not determine why I was not getting the correct output. Upon formalising the algorithm and attempting the proove its correctness I realised that I had made a wrong assumption. After this I determined that I must only check the children of children that contribute to a maximum w-path.

The only part of the dissertation I that is not as well well developed is the empirical study. Initially that chapter was supposed to be the backbone of the dissertation, espcially if I had failed to produce meaningful results with the extended persistence. After commiting so much time to the w-detection algorithms and extended persistence I had already covered too much ground and had little time to produce a more detailed and insighful empirical study.

Overall the project evolved as I was working on it and my ideas for what exacly I would put in it chanded throughout the course of he summer. This approach was more risky than I would have liked, but in the end I was confident in my ability to finish it as I had planned. The project also turned out to be more theoretical than I had originally planned. I am satisfied that this is the case. This gave me the opportunity to advance my mathematical and analytical skills. It has also opened the door for more future research directions and made me more flexible.

% Overall I feel that my approach to doing was not optimal. I struggled in the begining and overcommited alot of my time and energy to it.


\section{Future Work}

Finally we will present possile direction for extending this empirical study:

\subsection{Practical}

\begin{itemize}
    \item One obvious direction is to extend the empirical study by ontaining more real life data to analyse.

    \item Can we obtain the w-diameter of a contour tree directly from raw data? Computing the w-diameter of the contour tree itself is useful, but it is like port mortem analysis. If we are able to obtain the w-diameter without constructing the contour tree explicitly we may be able to use the information to speed up the contruction around the w-diameter.

    \item Are there other ways of generating w-structures? Can we devise a general way that would work in any number of dimensions?

    \item We did not have a chance to investigate how different w-structures interact with one another during the merge phase. This will probably be the next best step.

\end{itemize}

\subsection{Theoretical}

There are many more unansweared question on the relations of contour tree simplification and persistent homology. For example notice that the way the 0th persistence of an ascending filtration is computed is similar to how a join tree is computer. The similarity is in that joining components correspond to connected components in sublevel sets which in turn correspond to homology classes in the 0th homology. The joining of components coresponds to merging of homology classes. Consequently the persistent pairs correspond to branches in the join tree. If we instead take a descending filtration we obtain a similarity between the split tree and the persistent homology of the descending filtration. Further work can be done in showing whether the computations are equivalent formally and in translating idea from each of the frameworks to the other.

Another question is whether contour tree simplification can be expressed through through some filtration similar to persistent homology. Consider for example a simplicial mesh $M$ and its level sets $M_i$. Homology gives us to tools of identifying the connected components of the the $M_i$ by computing the 0th homology. It is possible however to somehow relate the homology classes of different level sets. This would require a sequence of the form

$$ ... \rightarrow H_0(M_i) \rightarrow H_0(M_{i+1}) \rightarrow ... \rightarrow H_0(M_{j}) \rightarrow H_0(M_{j+1}) \rightarrow ...$$

But what would the function between the homology groups be? Can me somehow induce them through function between the sublevel sets themselvs? We certainly cannot use inclusion maps because level sets are not neseted. It is worth contemplating whether this is feasble. Another question is whether it would be practical at all. Even if we do so all we will obtain is a mathematical reformulation of a problem we already have. Will that lead to more efficient algorithms or to better understanding of the subject?

We are not able to expand more on questions like these and leave them as subject for further study.






% This is the conclusion.
%
% We have done so many things.
%
% I tried so hard
% And got so far
% But in the end
% It doesn't even matter
% I had to fall
% To lose it all
% But in the end
% It doesn't even matter
