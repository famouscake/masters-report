\chapter{Empirical Study}
\label{chapter3}

*Whole Chapter is Under construction* 

The main objective of the empirical study if two fold. Firstly to demonstrate that w-structures are found in real world data. Secondly to correlate the iterations needed to collapse the whole CT with the w-diameter. I am doing this to better understand the w-structures and their effect on the algorithm. The hope is that more detailed understanding might leads us to find out whether they simply pose an implementation problem or a major algorithmic difficulty.

The main objective spawned additional objectives like:

\begin{itemize}
    \item Determine the smallest 2-dimensional grid dataset that exhibits a w-structure.
    \item Determine whether it is possible to determine the w-diameter of a datasets without computing it's contour tree.
    \item What kinds of structures in input data produce CTs with large w-diameters.
\end{itemize}



\section{Algorithms Implemented}

For this dissertation I first implemented the algorithm for computing the Contour Tree of a rectangular 2-dimensional mesh. I took the pseudocode from [] and implemented it in C++. To test my code and establish ground truth Hamish Carr provided me with his old cold which I compiled and ran against. I did not try to do any optimisations on the code I just implemented it to understand how it works.

After this I implemented three algorithm for w-detection. The first one is the most basic exhaustive approach where we find the w-length of all paths in the tree. This has quadratic running time. The next one was the doubleBFS algorithm and then the DP algorihtm. As previous they were implemented in C++. I have attempted no specific low optimisation I just wanted to make sure they work and test them on reasonably large data sets.

Another algorithm I implemented is one that generates all possible $n \times m$ 2-d grids. The objective here is to find the minimum one that produces a w-structure.

Lastly I implemented the algorithm in [] that computes extended persistence. I did this to verify the claims that I will make in the next chapter about the pairing of the global minimum and maximum.

All of the implementation are serial and not attempt has been made to parallelise them. There was simply no point in that as it is not the main objective of this dissertation.

I have also used several third party applications. The first one is Hamish Carr's serial and parallel implementations of the CT. I have also used software that computer persistent homology called Perseus* to test the correctness of the implementation of the one I have.

I don't know if this is relevant but the implementational side of things took around 2 weeks of full time work. 

\subsection{Running Times}

Here I will show that the algorithm running time are consistent with theoretical results from the previous chapter. The biggest contribution here is to show the DP algorithm does in indeed scale close to linearly as opposed to the quadratic upper bound provided in the previous chapter.

\section{Analysing Datasets}

I will analyse the following datasets. They are amazing and spectacular and phenomenal. They are unyielding! 

The table will include data sets, their diameter, their w-diameter, itteration needed to collapse.

Also find some data sets to analyse. Maybe do some medical 3-dimentional data sets.

Talk about why random data sets may not be completely reliable.

*Show some graphs and shit*

*Make some reflective summary of scheize*

\section{Finding the smallest W-structure}

This has educational value. It's also useful for out general understanding.

\section{Future work for the empirical study.}

Summarise things say what was successful, what was not. That kind of stuff.

This chapter does seem short. This is because most of the work put in the dissertation has either been theoretical which is in the previous chapters. or on impelmenting the newly created algorithms, which are in the appendix.

